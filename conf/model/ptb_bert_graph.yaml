# @package _global_
d_model: 2048

encoder: bert-large-uncased
use_tags: false
use_words: false
d_kqv: 64
d_ff: 2048
word_emb_dropout: 0.3
tag_emb_dropout: 0
relu_dropout: 0.2
residual_dropout: 0
attention_dropout: 0
num_attn_layers: 4
num_attn_heads: 8

decoder: graph
num_gcn_layers: 4

learning_rate: 3e-5
weight_decay: 0
subbatch_max_tokens: 700
